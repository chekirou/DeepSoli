@inproceedings{Chen2013,
abstract = {While much progress has been made in wearable computing in recent years, input techniques remain a key challenge. In this paper, we introduce uTrack, a technique to convert the thumb and fingers into a 3D input system using magnetic field (MF) sensing. A user wears a pair of magnetometers on the back of their fingers and a permanent magnet affixed to the back of the thumb. By moving the thumb across the fingers, we obtain a continuous input stream that can be used for 3D pointing. Specifically, our novel algorithm calculates the magnet's 3D position and tilt angle directly from the sensor readings. We evaluated uTrack as an input device, showing an average tracking accuracy of 4.84 mm in 3D space - sufficient for subtle interaction. We also demonstrate a real-time prototype and example applications allowing users to interact with the computer using 3D finger input. Copyright {\textcopyright} 2013 ACM.},
author = {Chen, Ke Yu and Lyons, Kent and White, Sean and Patel, Shwetak},
booktitle = {UIST 2013 - Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2501988.2502035},
isbn = {9781450322683},
keywords = {3D,Finger,Interaction,MF,Magnetic-field,Pointing,Sensing},
pages = {237--244},
title = {{UTrack: 3D input using two magnetic sensors}},
year = {2013}
}
@article{Choi2019,
abstract = {Due to the development of short-range radar with high-resolution, the radar sensor has a high potential to be used in real human-computer interaction (HCI) applications. The radar sensor has advantages over optical cameras in that it is unaffected by illumination and it is able to detect the objects in an occluded environment. This paper proposes a hand gesture recognition system for a real-time application of HCI using 60 GHz frequency-modulated continuous wave (FMCW) radar, Soli, developed by Google. The overall system includes signal processing part that generates range-Doppler map (RDM) sequences without clutter and machine learning part including a long short-term memory (LSTM) encoder to learn the temporal characteristics of the RDM sequences. A set of data is collected from 10 participants for the experiment. The proposed hand gesture recognition system successfully distinguishes 10 gestures with a high classification accuracy of 99.10{\%}. It also recognizes the gestures of a new participant with an accuracy of 98.48{\%}.},
annote = {ils ont l'air d'avoir copi{\'{e}} notre papier mais avec moins de gestesÂ  mais ils peuvrent etre utiles pour la redact
ions.},
author = {Choi, Jae Woo and Ryu, Si Jung and Kim, Jong Hwan},
doi = {10.1109/ACCESS.2019.2903586},
issn = {21693536},
journal = {IEEE Access},
keywords = {FMCW radar,LSTM encoder,gesture recognitio,machine learning,real-time interaction},
pages = {33610--33618},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Short-Range Radar Based Real-Time Hand Gesture Recognition Using LSTM Encoder}},
volume = {7},
year = {2019}
}
@inproceedings{Darrell1993,
abstract = {This paper presents a method for learning, tracking, and recognizing human gestures using a view-based approach to model articulated objects. Objects are represented using sets of view models, rather than single templates. Stereotypical space-time patterns, i.e. gestures, are then matched to stored gesture patterns using dynamic time warping. Real-time performance is achieved by using special-purpose correlation hardware and view prediction to prune as much of the search space as possible. Both view models and view predictions are learned from examples. We present results showing tracking and recognition of human hand gesture at over 10Hz.},
author = {Darrell, Trevor and Pentland, Alex},
booktitle = {IEEE Computer Vision and Pattern Recognition},
doi = {10.1109/cvpr.1993.341109},
isbn = {0818638826},
pages = {335--340},
publisher = {Publ by IEEE},
title = {{Space-time gestures}},
year = {1993}
}
@article{Kim2016,
abstract = {In this paper, we investigate the feasibility of recognizing human hand gestures using micro-Doppler signatures measured by Doppler radar with a deep convolutional neural network (DCNN). Hand gesture recognition using radar can be applied to control electronic appliances. Compared with an optical recognition system, radar can work regardless of light conditions and it can be embedded in a case. We classify ten different hand gestures, with only micro-Doppler signatures on spectrograms without range information. The ten gestures, which included swiping from left to right, swiping from right to left, rotating clockwise, rotating counterclockwise, pushing, double pushing, holding, and double holding, were measured using Doppler radar and their spectrograms investigated. A DCNN was employed to classify the spectrograms, with 90{\%} of the data utilized for training and the remaining 10{\%} for validation. After five-fold validation, the classification accuracy of the proposed method was found to be 85.6{\%}. With seven gestures, the accuracy increased to 93.1{\%}.},
author = {Kim, Youngwook and Toomajian, Brian},
doi = {10.1109/ACCESS.2016.2617282},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Deep convolutional neural networks,Doppler radar,Hand gesture,micro-Doppler signatures},
pages = {7125--7130},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Hand Gesture Recognition Using Micro-Doppler Signatures with Convolutional Neural Network}},
volume = {4},
year = {2016}
}
@article{Lee1999,
abstract = {The task of automatic gesture recognition is highly challenging due to the presence of unpredictable and ambiguous nongesture hand motions. In this paper, a new method is developed using the Hidden Markov Model based technique. To handle nongesture patterns, we introduce the concept of a threshold model that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns. The threshold model is a weak model for all trained gestures in the sense that its likelihood is smaller than that of the dedicated gesture model for a given gesture. Consequently, the likelihood can be used as an adaptive threshold for selecting proper gesture model. It has, however, a large number of states and needs to be reduced because the threshold model is constructed by collecting the states of all gesture models in the system. To overcome this problem, the states with similar probability distributions are merged, utilizing the relative entropy measure. Experimental results show that the proposed method can successfully extract trained gestures from continuous hand motion with 93.14 percent reliability. {\textcopyright} 1999 IEEE.},
author = {Lee, Hyeon Kyu and Kim, Jin H.},
doi = {10.1109/34.799904},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Gesture spotting,Hand gesture,Hidden Markov Model,Pattern recognition,Relative entropy,Segmentation,State reduction,Threshold model},
number = {10},
pages = {961--973},
title = {{An HMM-Based threshold model approach for gesture recognition}},
volume = {21},
year = {1999}
}
@inproceedings{Lien2016,
abstract = {This paper presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solidstate radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.},
author = {Lien, Jaime and Gillian, Nicholas and Karagozler, M. Emre and Amihood, Patrick and Schwesig, Carsten and Olson, Erik and Raja, Hakim and Poupyrev, Ivan},
booktitle = {ACM Transactions on Graphics},
doi = {10.1145/2897824.2925953},
file = {::},
issn = {15577368},
keywords = {Gestures,Interaction,RF,Radar,Sensors},
month = {jul},
number = {4},
pages = {1--19},
publisher = {Association for Computing Machinery},
title = {{Soli: Ubiquitous gesture sensing with millimeter wave radar}},
url = {http://dl.acm.org/citation.cfm?doid=2897824.2925953},
volume = {35},
year = {2016}
}
@inproceedings{Nguyen2018,
abstract = {Touchless hand gesture using portable millimeter-wave radar sensors an enabling technology for Internet of Things (IoT) applications. In this paper, we investigate the feasibility of using a frequency modulated continuous wave (FMCW) radar with noise removal and range gating method to recognize human hand gesture for a user moving in the radar's field of view. These detected hand gestures will be applied to remote control of computers or smart TVs at a distance from 0.3 m to 1.2 m.},
author = {Nguyen, Minh Q. and Flores-Nigaglioni, Anthony and Li, Changzhi},
booktitle = {2018 IEEE MTT-S International Wireless Symposium, IWS 2018 - Proceedings},
doi = {10.1109/IEEE-IWS.2018.8400811},
isbn = {9781538663462},
keywords = {FMCW radar,IoT applications,Touchless hand gesture,noise removal method,range gating method},
month = {jun},
pages = {1--4},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Range-gating technology for millimeter-wave radar remote gesture control in IoT applications}},
year = {2018}
}
@inproceedings{Pu2013,
abstract = {This paper presents WiSee, a novel gesture recognition system that leverages wireless signals (e.g., Wi- Fi) to enable whole-home sensing and recognition of human gestures. Since wireless signals do not require line-of-sight and can traverse through walls, WiSee can enable wholehome gesture recognition using few wireless sources. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We implement a proof-ofconcept prototype of WiSee using USRP-N210s and evaluate it in both an office environment and a two-bedroom apartment. Our results show that WiSee can identify and classify a set of nine gestures with an average accuracy of 94{\%}. {\textcopyright} 2013 by the Association for Computing Machinery, Inc.},
address = {New York, New York, USA},
author = {Pu, Qifan and Gupta, Sidhant and Gollakota, Shyamnath and Patel, Shwetak},
booktitle = {Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM},
doi = {10.1145/2500423.2500436},
file = {::},
isbn = {9781450319997},
keywords = {Gesture recognition,Wireless sensing},
pages = {27--38},
publisher = {ACM Press},
title = {{Whole-home gesture recognition using wireless signals}},
url = {http://dl.acm.org/citation.cfm?doid=2500423.2500436},
year = {2013}
}
@article{Simonyan2014,
abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
archivePrefix = {arXiv},
arxivId = {1406.2199},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1406.2199},
file = {:home/celina/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recognition in Videos.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
number = {January},
pages = {568--576},
publisher = {Neural information processing systems foundation},
title = {{Two-Stream Convolutional Networks for Action Recognition in Videos}},
url = {http://arxiv.org/abs/1406.2199},
volume = {1},
year = {2014}
}
@inproceedings{Wan2014,
abstract = {In this article, we consider the design of a human gesture recognition system based on pattern recognition of signatures from a portable smart radar sensor. Powered by AAA batteries, the smart radar sensor operates in the 2.4 GHz industrial, scientific and medical (ISM) band. We analyzed the feature space using principle components and application-specific time and frequency domain features extracted from radar signals for two different sets of gestures. We illustrate that a nearest neighbor based classifier can achieve greater than 95{\%} accuracy for multi class classification using 10 fold cross validation when features are extracted based on magnitude differences and Doppler shifts as compared to features extracted through orthogonal transformations. The reported results illustrate the potential of intelligent radars integrated with a pattern recognition system for high accuracy smart home and health monitoring purposes.},
author = {Wan, Qian and Li, Yiran and Li, Changzhi and Pal, Ranadip},
booktitle = {2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014},
doi = {10.1109/EMBC.2014.6945096},
isbn = {9781424479290},
month = {nov},
pages = {6414--6417},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Gesture recognition for smart home applications using portable radar sensors}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25571464},
volume = {2014},
year = {2014}
}
@inproceedings{Wang2016,
abstract = {This paper proposes a novel machine learning architecture, specifically designed for radio-frequency based gesture recognition. We focus on high-frequency (60 GHz), shortrange radar based sensing, in particular Google's Soli sensor. The signal has unique properties such as resolving motion at a very fine level and allowing for segmentation in range and velocity spaces rather than image space. This enables recognition of new types of inputs but poses significant difficulties for the design of input recognition algorithms. The proposed algorithm is capable of detecting a rich set of dynamic gestures and can resolve small motions of fingers in fine detail. Our technique is based on an end-to-end trained combination of deep convolutional and recurrent neural networks. The algorithm achieves high recognition rates (avg 87{\%}) on a challenging set of 11 dynamic gestures and generalizes well across 10 users. The proposed model runs on commodity hardware at 140Hz (CPU only).},
author = {Wang, Saiwen and Song, Jie and Lien, Jamie and Poupyrev, Ivan and Hilliges, Otmar},
booktitle = {UIST 2016 - Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
doi = {10.1145/2984511.2984565},
isbn = {9781450345316},
keywords = {Deep learning,Gesture recognition,Radar sensing,Wearables},
month = {oct},
pages = {851--860},
publisher = {Association for Computing Machinery, Inc},
title = {{Interacting with soli: Exploring fine-grained dynamic gesture recognition in the radio-frequency spectrum}},
year = {2016}
}
@article{Zhang2017,
abstract = {Hand gesture recognition has long been a hot topic in human computer interaction. Traditional camera-based hand gesture recognition systems cannot work properly under dark circumstances. In this paper, a Doppler Radar based hand gesture recognition system using convolutional neural networks is proposed. A cost-effective Doppler radar sensor with dual receiving channels at 5.8GHz is used to acquire a big database of four standard gestures. The received hand gesture signals are then processed with time-frequency analysis. Convolutional neural networks are used to classify different gestures. Experimental results verify the effectiveness of the system with an accuracy of 98{\%}. Besides, related factors such as recognition distance and gesture scale are investigated.},
annote = {celui la est tres interesant.},
archivePrefix = {arXiv},
arxivId = {1711.02254},
author = {Zhang, Jiajun and Tao, Jinkun and Huangfu, Jiangtao and Shi, Zhiguo},
eprint = {1711.02254},
file = {::},
journal = {Lecture Notes in Electrical Engineering},
month = {nov},
pages = {1096--1113},
publisher = {Springer Verlag},
title = {{Doppler-Radar Based Hand Gesture Recognition System Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1711.02254},
volume = {463},
year = {2017}
}
@inproceedings{Zhang2017a,
abstract = {Dynamic hand gesture recognition is of great importance for human-computer interaction. In this paper, we present a method to discriminate the four kinds of dynamic hand gestures, snapping fingers, flipping fingers, hand rotation and calling, using a radar micro-Doppler sensor. Two micro-Doppler features are extracted from the time-frequency spectrum and the support vector machine is used to classify these four kinds of gestures. The experimental results on measured data demonstrate that the proposed method can produce a classification accuracy higher than 88.56{\%}.},
author = {Zhang, Shimeng and Li, Gang and Ritchie, Matthew and Fioranelli, Francesco and Griffiths, Hugh},
booktitle = {2016 CIE International Conference on Radar, RADAR 2016},
doi = {10.1109/RADAR.2016.8059518},
isbn = {9781509048281},
keywords = {Hand gesture classification,Human-computer interaction,Micro-Doppler signatures,Support vector machine},
month = {oct},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Dynamic hand gesture classification based on radar micro-Doppler signatures}},
year = {2017}
}
@article{Zhang2018,
abstract = {Recently, hand gesture recognition systems have become increasingly interesting to researchers in the field of human-computer interfaces. Real-world systems for human dynamic hand gesture recognition is challenging as: 1) the system must be robust to various conditions; 2) there is a rich diversity in how people perform hand gestures, making hand gesture recognition difficult; and 3) the system must detect and recognize hand gestures continuously using unsegmented input streams in order to avoid noticeable lag between performing a gesture and its classification. In this paper, to address these challenges, we present Latern, a novel system for dynamic continuous hand gesture recognition based on a frequency-modulated continuous wave radar sensor. The radar system does not depend on lighting, noise, or atmospheric conditions. We employ a recurrent 3-D convolutional neural network to perform the classification of dynamic hand gestures. To enhance the processing performance, a connectionist temporal classification algorithm is used to train the network to predict class labels from inprogress gestures in unsegmented input streams. The experimental results show that Latern is able to achieve high recognition rates of 96{\%}, which is higher than state-of-the-art hand gesture recognition systems. In addition, the conclusion in this paper can be used for a real-time hand gesture recognition system design.},
author = {Zhang, Zhenyuan and Tian, Zengshan and Zhou, Mu},
doi = {10.1109/JSEN.2018.2808688},
issn = {1530437X},
journal = {IEEE Sensors Journal},
keywords = {FMCW radar,Hand gesture recognition,connectionist temporal classification,recurrent three-dimensional convolutional neural n},
month = {apr},
number = {8},
pages = {3278--3289},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Latern: Dynamic Continuous Hand Gesture Recognition Using FMCW Radar Sensor}},
volume = {18},
year = {2018}
}
@inproceedings{Zhao2014,
abstract = {Current smartphone inputs are limited to physical buttons, touchscreens, cameras or built-in sensors. These approaches either require a dedicated surface or line-of-sight for interaction. We introduce SideSwipe, a novel system that enables in-air gestures both above and around a mobile device. Our system leverages the actual (unmodified) GSM signal to detect hand gestures around the device. We developed an algorithm to convert the bursty reflected GSM pulses to a continuous signal that can be used for gesture recognition. Specifically, when a user waves their hand near the phone, the hand movement disturbs the signal propagation between the phone's transmitter and added receiving antennas. Our system captures this variation and uses it for gesture recognition. To evaluate our system, we conduct a study with 10 participants and present robust gesture recognition with an average accuracy of 87.2{\%} across 14 hand gestures.},
address = {New York, New York, USA},
author = {Zhao, Chen and Chen, Ke Yu and Aumi, Md Tanvir Islam and Patel, Shwetak and Reynolds, Matthew S.},
booktitle = {UIST 2014 - Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2642918.2647380},
file = {::},
isbn = {9781450330695},
keywords = {Antenna,GSM,In-air hand gesture},
month = {oct},
pages = {527--534},
publisher = {Association for Computing Machinery, Inc},
title = {{SideSwipe: Detecting in-air gestures around mobile devices using actual GSM signals}},
url = {http://dl.acm.org/citation.cfm?doid=2642918.2647380},
year = {2014}
}
