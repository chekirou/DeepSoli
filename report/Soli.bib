Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Erol2007,
abstract = {Direct use of the hand as an input device is an attractive method for providing natural human-computer interaction (HCI). Currently, the only technology that satisfies the advanced requirements of hand-based input for HCI is glove-based sensing. This technology, however , has several drawbacks including that it hinders the ease and naturalness with which the user can interact with the computer-controlled environment, and it requires long calibration and setup procedures. Computer vision (CV) has the potential to provide more natural, non-contact solutions. As a result, there have been considerable research efforts to use the hand as an input device for HCI. In particular, two types of research directions have emerged. One is based on gesture classification and aims to extract high-level abstract information corresponding to motion patterns or postures of the hand. The second is based on pose estimation systems and aims to capture the real 3D motion of the hand. This paper presents a literature review on the latter research direction, which is a very challenging problem in the context of HCI.},
author = {Erol, Ali and Bebis, George and Nicolescu, Mircea and Boyle, Richard D and Twombly, Xander},
doi = {10.1016/j.cviu.2006.10.012},
file = {:home/chekirou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erol et al. - 2007 - Vision-based hand pose estimation A review(2).pdf:pdf},
keywords = {Gesture recognition,Gesture-based HCI,Hand pose estimation},
title = {{Vision-based hand pose estimation: A review}},
url = {www.elsevier.com/locate/cviu},
year = {2007}
}
@article{Pu,
abstract = {This paper presents WiSee, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures. Since wireless signals do not require line-of-sight and can traverse through walls, WiSee can enable whole-home gesture recognition using few wireless sources. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We implement a proof-of-concept prototype of WiSee using USRP-N210s and evaluate it in both an office environment and a two-bedroom apartment. Our results show that WiSee can identify and classify a set of nine gestures with an average accuracy of 94{\%}.},
author = {Pu, Qifan and Gupta, Sidhant and Gollakota, Shyamnath and Patel, Shwetak},
doi = {10.1145/2500423.2500436},
file = {:home/chekirou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pu et al. - Unknown - Whole-Home Gesture Recognition Using Wireless Signals(3).pdf:pdf},
isbn = {9781450319997},
keywords = {()},
title = {{Whole-Home Gesture Recognition Using Wireless Signals}}
}
@inproceedings{Hwang2013,
abstract = {This paper proposes user-customizable passive control widgets, called MagGetz, which enable tangible interaction on and around mobile devices without requiring power or wireless connections. This is achieved by tracking and analyzing the magnetic field generated by controllers attached on and around the device through a single magnetometer, which is commonly integrated in smartphones today. The proposed method provides users with a broader interaction area, customizable input layouts, richer physical clues, and higher input expressiveness without the need for hardware modifications. We have presented a software toolkit and several applications using MagGetz. Copyright {\textcopyright} 2013 ACM.},
address = {New York, New York, USA},
author = {Hwang, Sungjae and Ahn, Myungwook and Wohn, Kwang Yun},
booktitle = {UIST 2013 - Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2501988.2501991},
isbn = {9781450322683},
keywords = {Around device interaction (ADI),Magnetometer,Mobile device,Pressure input,Tangible user interface (TUI)},
pages = {411--416},
publisher = {ACM Press},
title = {{MagGetz: Customizable passive tangible controllers on and around conventional mobile devices}},
url = {http://dl.acm.org/citation.cfm?doid=2501988.2501991},
year = {2013}
}
@inproceedings{Sharp2015,
abstract = {We present a new real-time hand tracking system based on a single depth camera. The system can accurately reconstruct complex hand poses across a variety of subjects. It also allows for robust tracking, rapidly recovering from any temporary failures. Most uniquely, our tracker is highly flexible, dramatically improving upon previous approaches which have focused on front-facing close-range scenarios. This flexibility opens up new possibilities for human-computer interaction with examples including tracking at distances from tens of centimeters through to several meters (for controlling the TV at a distance), supporting tracking using a moving depth camera (for mobile scenarios), and arbitrary camera placements (for VR headsets). These features are achieved through a new pipeline that combines a multi-layered discriminative reinitialization strategy for per-frame pose estimation, followed by a generative model-fitting stage. We provide extensive technical details and a detailed qualitative and quantitative analysis.},
author = {Sharp, Toby and Keskin, Cem and Robertson, Duncan and Taylor, Jonathan and Shotton, Jamie and Kim, David and Rhemann, Christoph and Leichter, Ido and Vinnikov, Alon and Wei, Yichen and Freedman, Daniel and Kohli, Pushmeet and Krupka, Eyal and Fitzgibbon, Andrew and Izadi, Shahram},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2702123.2702179},
isbn = {9781450331456},
month = {apr},
pages = {3633--3642},
publisher = {Association for Computing Machinery},
title = {{Accurate, robust, and flexible realtime hand tracking}},
volume = {2015-April},
year = {2015}
}
@article{Wang,
abstract = {Figure 1. We explore interactive possibilities enabled by Google's project Soli (A), a solid-state short-range radar, capturing energy reflected of hands and other objects (B). The signal is unique in that it resolves motion in the millimeter range but does not directly capture shape (C). We propose a novel gesture recognition algorithm specifically designed to recognize subtle, low-effort gestures based on the Soli signal. ABSTRACT This paper proposes a novel machine learning architecture , specifically designed for radio-frequency based gesture recognition. We focus on high-frequency (60 GHz), short-range radar based sensing, in particular Google's Soli sensor. The signal has unique properties such as resolving motion at a very fine level and allowing for segmentation in range and velocity spaces rather than image space. This enables recognition of new types of inputs but poses significant difficulties for the design of input recognition algorithms. The proposed algorithm is capable of detecting a rich set of dynamic gestures and can resolve small motions of fingers in fine detail. Our technique is based on an end-to-end trained combination of deep convolutional and recurrent neural networks. The algorithm achieves high recognition rates (avg 87{\%}) on a challenging set of 11 dynamic gestures and generalizes well across 10 users. The proposed model runs on commodity hardware at 140 Hz (CPU only).},
author = {Wang, Saiwen and Song, Jie and Lien, Jaime and Poupyrev, Ivan and Hilliges, Otmar and Zurich, Eth and Atap, Google},
doi = {10.1145/2984511.2984565},
file = {:home/chekirou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Interacting with Soli Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum(2).pdf:pdf},
isbn = {9781450341899},
keywords = {Author Keywords gesture recognition,I36 Methodology and Techniques,deep learning,radar sensing ACM Classification Keywords H52 User,wearables,†},
title = {{Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum}},
url = {http://dx.doi.org/10.1145/2984511.2984565}
}
@inproceedings{Lien2016,
abstract = {This paper presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solidstate radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.},
author = {Lien, Jaime and Gillian, Nicholas and Karagozler, M. Emre and Amihood, Patrick and Schwesig, Carsten and Olson, Erik and Raja, Hakim and Poupyrev, Ivan},
booktitle = {ACM Transactions on Graphics},
doi = {10.1145/2897824.2925953},
file = {:home/chekirou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lien et al. - 2016 - Soli Ubiquitous gesture sensing with millimeter wave radar(2).pdf:pdf},
issn = {15577368},
keywords = {Gestures,Interaction,RF,Radar,Sensors},
month = {jul},
number = {4},
pages = {1--19},
publisher = {Association for Computing Machinery},
title = {{Soli: Ubiquitous gesture sensing with millimeter wave radar}},
url = {http://dl.acm.org/citation.cfm?doid=2897824.2925953},
volume = {35},
year = {2016}
}
@book{Kim,
abstract = {Digits is a wrist-worn sensor that recovers the full 3D pose of the user's hand. This enables a variety of freehand interactions on the move. The system targets mobile settings, and is specifically designed to be low-power and easily reproducible using only off-the-shelf hardware. The electronics are self-contained on the user's wrist, but optically image the entirety of the user's hand. This data is processed using a new pipeline that robustly samples key parts of the hand, such as the tips and lower regions of each finger. These sparse samples are fed into new kinematic models that leverage the biomechanical constraints of the hand to recover the 3D pose of the user's hand. The proposed system works without the need for full instrumentation of the hand (for example using data gloves), additional sensors in the environment, or depth cameras which are currently prohibitive for mobile scenarios due to power and form-factor considerations. We demonstrate the utility of Digits for a variety of application scenarios , including 3D spatial interaction with mobile devices, eyes-free interaction on-the-move, and gaming. We conclude with a quantitative and qualitative evaluation of our system, and discussion of strengths, limitations and future work.},
author = {Kim, David and Hilliges, Otmar and Izadi, Shahram and Butler, Alex and Chen, Jiawen and Oikonomidis, Iason and Olivier, Patrick},
isbn = {9781450315807},
keywords = {UIST},
title = {{Digits: Freehand 3D Interactions Anywhere Using a Wrist-Worn Gloveless Sensor}}
}
@inproceedings{Nguyen2018,
abstract = {Touchless hand gesture using portable millimeter-wave radar sensors an enabling technology for Internet of Things (IoT) applications. In this paper, we investigate the feasibility of using a frequency modulated continuous wave (FMCW) radar with noise removal and range gating method to recognize human hand gesture for a user moving in the radar's field of view. These detected hand gestures will be applied to remote control of computers or smart TVs at a distance from 0.3 m to 1.2 m.},
author = {Nguyen, Minh Q. and Flores-Nigaglioni, Anthony and Li, Changzhi},
booktitle = {2018 IEEE MTT-S International Wireless Symposium, IWS 2018 - Proceedings},
doi = {10.1109/IEEE-IWS.2018.8400811},
isbn = {9781538663462},
keywords = {FMCW radar,IoT applications,Touchless hand gesture,noise removal method,range gating method},
month = {jun},
pages = {1--4},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Range-gating technology for millimeter-wave radar remote gesture control in IoT applications}},
year = {2018}
}
@inproceedings{Gupta2012,
abstract = {Gesture is becoming an increasingly popular means of interacting with computers. However, it is still relatively costly to deploy robust gesture recognition sensors in existing mobile platforms. We present SoundWave, a technique that leverages the speaker and microphone already embedded in most commodity devices to sense in-air gestures around the device. To do this, we generate an inaudible tone, which gets frequency-shifted when it reflects off moving objects like the hand. We measure this shift with the microphone to infer various gestures. In this note, we describe the phenomena and detection algorithm, demonstrate a variety of gestures, and present an informal evaluation on the robustness of this approach across different devices and people. Copyright 2012 ACM.},
address = {New York, New York, USA},
author = {Gupta, Sidhant and Morris, Dan and Patel, Shwetak N. and Tan, Desney},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2207676.2208331},
isbn = {9781450310154},
keywords = {Doppler,In-air gesture sensing,Interaction technique},
pages = {1911--1914},
publisher = {ACM Press},
title = {{SoundWave: Using the Doppler effect to sense gestures}},
url = {http://dl.acm.org/citation.cfm?doid=2207676.2208331},
year = {2012}
}
@inproceedings{Keskin2012,
abstract = {Vision based articulated hand pose estimation and hand shape classification are challenging problems. This paper proposes novel algorithms to perform these tasks using depth sensors. In particular, we introduce a novel randomized decision forest (RDF) based hand shape classifier, and use it in a novel multi-layered RDF framework for articulated hand pose estimation. This classifier assigns the input depth pixels to hand shape classes, and directs them to the corresponding hand pose estimators trained specifically for that hand shape. We introduce two novel types of multi-layered RDFs: Global Expert Network (GEN) and Local Expert Network (LEN), which achieve significantly better hand pose estimates than a single-layered skeleton estimator and generalize better to previously unseen hand poses. The novel hand shape classifier is also shown to be accurate and fast. The methods run in real-time on the CPU, and can be ported to the GPU for further increase in speed. {\textcopyright} 2012 Springer-Verlag.},
author = {Keskin, Cem and Kira{\c{c}}, Furkan and Kara, Yunus Emre and Akarun, Lale},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33783-3_61},
isbn = {9783642337826},
issn = {03029743},
number = {PART 6},
pages = {852--863},
publisher = {Springer, Berlin, Heidelberg},
title = {{Hand pose estimation and hand shape classification using multi-layered randomized decision forests}},
volume = {7577 LNCS},
year = {2012}
}
@inproceedings{Song2014,
abstract = {We present a novel machine learning based algorithm extending the interaction space around mobile devices. The technique uses only the RGB camera now commonplace on off-the-shelf mobile devices. Our algorithm robustly recognizes a wide range of in-air gestures, supporting user variation, and varying lighting conditions. We demonstrate that our algorithm runs in real-time on unmodified mobile devices, including resource-constrained smartphones and smartwatches. Our goal is not to replace the touchscreen as primary input device, but rather to augment and enrich the existing interaction vocabulary using gestures. While touch input works well for many scenarios, we demonstrate numerous interaction tasks such as mode switches, application and task management, menu selection and certain types of navigation, where such input can be either complemented or better served by inair gestures. This removes screen real-estate issues on small touchscreens, and allows input to be expanded to the 3D space around the device. We present results for recognition accuracy (93{\%} test and 98{\%} train), impact of memory footprint and other model parameters. Finally, we report results from preliminary user evaluations, discuss advantages and limitations and conclude with directions for future work.},
author = {Song, Jie and S{\"{o}}r{\"{o}}s, G{\'{a}}bor and Pece, Fabrizio and Fanello, Sean Ryan and Izadi, Shahram and Keskin, Cem and Hilliges, Otmar},
booktitle = {UIST 2014 - Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2642918.2647373},
isbn = {9781450330695},
keywords = {Gesture recognition,HCI,Mobile computing,Mobile gestures,Mobile interaction,Random forests},
month = {oct},
pages = {319--330},
publisher = {Association for Computing Machinery, Inc},
title = {{In-air gestures around unmodified mobile devices}},
year = {2014}
}
@article{Kim2016,
abstract = {In this paper, we investigate the feasibility of recognizing human hand gestures using micro-Doppler signatures measured by Doppler radar with a deep convolutional neural network (DCNN). Hand gesture recognition using radar can be applied to control electronic appliances. Compared with an optical recognition system, radar can work regardless of light conditions and it can be embedded in a case. We classify ten different hand gestures, with only micro-Doppler signatures on spectrograms without range information. The ten gestures, which included swiping from left to right, swiping from right to left, rotating clockwise, rotating counterclockwise, pushing, double pushing, holding, and double holding, were measured using Doppler radar and their spectrograms investigated. A DCNN was employed to classify the spectrograms, with 90{\%} of the data utilized for training and the remaining 10{\%} for validation. After five-fold validation, the classification accuracy of the proposed method was found to be 85.6{\%}. With seven gestures, the accuracy increased to 93.1{\%}.},
author = {Kim, Youngwook and Toomajian, Brian},
doi = {10.1109/ACCESS.2016.2617282},
file = {:home/chekirou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Toomajian - 2016 - Hand Gesture Recognition Using Micro-Doppler Signatures with Convolutional Neural Network(2).pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Deep convolutional neural networks,Doppler radar,Hand gesture,micro-Doppler signatures},
pages = {7125--7130},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Hand Gesture Recognition Using Micro-Doppler Signatures with Convolutional Neural Network}},
volume = {4},
year = {2016}
}
@article{Choi2019,
abstract = {Due to the development of short-range radar with high-resolution, the radar sensor has a high potential to be used in real human-computer interaction (HCI) applications. The radar sensor has advantages over optical cameras in that it is unaffected by illumination and it is able to detect the objects in an occluded environment. This paper proposes a hand gesture recognition system for a real-time application of HCI using 60 GHz frequency-modulated continuous wave (FMCW) radar, Soli, developed by Google. The overall system includes signal processing part that generates range-Doppler map (RDM) sequences without clutter and machine learning part including a long short-term memory (LSTM) encoder to learn the temporal characteristics of the RDM sequences. A set of data is collected from 10 participants for the experiment. The proposed hand gesture recognition system successfully distinguishes 10 gestures with a high classification accuracy of 99.10{\%}. It also recognizes the gestures of a new participant with an accuracy of 98.48{\%}.},
annote = {ils ont l'air d'avoir copi{\'{e}} notre papier mais avec moins de gestes  mais ils peuvrent etre utiles pour la redact
ions.},
author = {Choi, Jae Woo and Ryu, Si Jung and Kim, Jong Hwan},
doi = {10.1109/ACCESS.2019.2903586},
issn = {21693536},
journal = {IEEE Access},
keywords = {FMCW radar,LSTM encoder,gesture recognitio,machine learning,real-time interaction},
pages = {33610--33618},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Short-Range Radar Based Real-Time Hand Gesture Recognition Using LSTM Encoder}},
volume = {7},
year = {2019}
}
@inproceedings{Wan2014,
abstract = {In this article, we consider the design of a human gesture recognition system based on pattern recognition of signatures from a portable smart radar sensor. Powered by AAA batteries, the smart radar sensor operates in the 2.4 GHz industrial, scientific and medical (ISM) band. We analyzed the feature space using principle components and application-specific time and frequency domain features extracted from radar signals for two different sets of gestures. We illustrate that a nearest neighbor based classifier can achieve greater than 95{\%} accuracy for multi class classification using 10 fold cross validation when features are extracted based on magnitude differences and Doppler shifts as compared to features extracted through orthogonal transformations. The reported results illustrate the potential of intelligent radars integrated with a pattern recognition system for high accuracy smart home and health monitoring purposes.},
author = {Wan, Qian and Li, Yiran and Li, Changzhi and Pal, Ranadip},
booktitle = {2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014},
doi = {10.1109/EMBC.2014.6945096},
isbn = {9781424479290},
month = {nov},
pages = {6414--6417},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Gesture recognition for smart home applications using portable radar sensors}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25571464},
volume = {2014},
year = {2014}
}
@inproceedings{Kim2012,
abstract = {Digits is a wrist-worn sensor that recovers the full 3D pose of the user's hand. This enables a variety of freehand interactions on the move. The system targets mobile settings, and is specifically designed to be low-power and easily reproducible using only off-the-shelf hardware. The electronics are self-contained on the user's wrist, but optically image the entirety of the user's hand. This data is processed using a new pipeline that robustly samples key parts of the hand, such as the tips and lower regions of each finger. These sparse samples are fed into new kinematic models that leverage the biomechanical constraints of the hand to recover the 3D pose of the user's hand. The proposed system works without the need for full instrumentation of the hand (for example using data gloves), additional sensors in the environment, or depth cameras which are currently prohibitive for mobile scenarios due to power and form-factor considerations. We demonstrate the utility of Digits for a variety of application scenarios, including 3D spatial interaction with mobile devices, eyes-free interaction on-the-move, and gaming. We conclude with a quantitative and qualitative evaluation of our system, and discussion of strengths, limitations and future work.},
address = {New York, New York, USA},
author = {Kim, David and Hilliges, Otmar and Izadi, Shahram and Butler, Alex D. and Chen, Jiawen and Oikonomidis, Iason and Olivier, Patrick},
booktitle = {Proceedings of the 25th annual ACM symposium on User interface software and technology - UIST '12},
doi = {10.1145/2380116.2380139},
pages = {167},
publisher = {Association for Computing Machinery (ACM)},
title = {{Digits}},
url = {http://dl.acm.org/citation.cfm?doid=2380116.2380139},
year = {2012}
}
@article{Zhang2018,
abstract = {Recently, hand gesture recognition systems have become increasingly interesting to researchers in the field of human-computer interfaces. Real-world systems for human dynamic hand gesture recognition is challenging as: 1) the system must be robust to various conditions; 2) there is a rich diversity in how people perform hand gestures, making hand gesture recognition difficult; and 3) the system must detect and recognize hand gestures continuously using unsegmented input streams in order to avoid noticeable lag between performing a gesture and its classification. In this paper, to address these challenges, we present Latern, a novel system for dynamic continuous hand gesture recognition based on a frequency-modulated continuous wave radar sensor. The radar system does not depend on lighting, noise, or atmospheric conditions. We employ a recurrent 3-D convolutional neural network to perform the classification of dynamic hand gestures. To enhance the processing performance, a connectionist temporal classification algorithm is used to train the network to predict class labels from inprogress gestures in unsegmented input streams. The experimental results show that Latern is able to achieve high recognition rates of 96{\%}, which is higher than state-of-the-art hand gesture recognition systems. In addition, the conclusion in this paper can be used for a real-time hand gesture recognition system design.},
author = {Zhang, Zhenyuan and Tian, Zengshan and Zhou, Mu},
doi = {10.1109/JSEN.2018.2808688},
issn = {1530437X},
journal = {IEEE Sensors Journal},
keywords = {FMCW radar,Hand gesture recognition,connectionist temporal classification,recurrent three-dimensional convolutional neural n},
month = {apr},
number = {8},
pages = {3278--3289},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Latern: Dynamic Continuous Hand Gesture Recognition Using FMCW Radar Sensor}},
volume = {18},
year = {2018}
}
@article{Zhang2017,
abstract = {Hand gesture recognition has long been a hot topic in human computer interaction. Traditional camera-based hand gesture recognition systems cannot work properly under dark circumstances. In this paper, a Doppler Radar based hand gesture recognition system using convolutional neural networks is proposed. A cost-effective Doppler radar sensor with dual receiving channels at 5.8GHz is used to acquire a big database of four standard gestures. The received hand gesture signals are then processed with time-frequency analysis. Convolutional neural networks are used to classify different gestures. Experimental results verify the effectiveness of the system with an accuracy of 98{\%}. Besides, related factors such as recognition distance and gesture scale are investigated.},
annote = {celui la est tres interesant.},
archivePrefix = {arXiv},
arxivId = {1711.02254},
author = {Zhang, Jiajun and Tao, Jinkun and Huangfu, Jiangtao and Shi, Zhiguo},
eprint = {1711.02254},
file = {:home/chekirou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Doppler-Radar Based Hand Gesture Recognition System Using Convolutional Neural Networks(2).pdf:pdf},
journal = {Lecture Notes in Electrical Engineering},
month = {nov},
pages = {1096--1113},
publisher = {Springer Verlag},
title = {{Doppler-Radar Based Hand Gesture Recognition System Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1711.02254},
volume = {463},
year = {2017}
}
