{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import sys,os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import utils.Dataset as u\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fc\n",
    "import torch.nn.modules.normalization as nm\n",
    "import torch.optim as optim\n",
    "import CNN as cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fonction split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = u.split(\"/home/celina/data/dsp/\" , use= 0.001) # return a split train test of the sequences, using all the data, 50% train, 50% test\n",
    "train2, test2 = u.split(\"/home/celina/data/dsp/\",frames = False, percentage = 0.8, use= 0.0001) # returns a split of the frames 80% train, 20% test, for 80% of the data\n",
    "train3, test3 = u.split(\"/home/celina/data/dsp/\",frames = False, already_defined= True)# returns train test split as defined in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/celina/data/dsp\"\n",
    "t = transforms.Compose([u.Reshape(), u.Rescale((224,224)), u.ToTensor()]) # composition of transformations\n",
    "data = u.Data(directory,train1,transform = t) # open the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utilisation du loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {'batch_size': 100, 'shuffle': True, 'num_workers': 6}\n",
    "loader = DataLoader(data,**loader_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ne pas executer , exemple type d'utilisation (avec cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Loop over epochs\\nfor epoch in range(max_epochs):\\n    # Training\\n    for local_batch, local_labels in training_generator:\\n        # Transfer to GPU\\n        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\\n        \\n\\n    # Validation\\n    with torch.set_grad_enabled(False):\\n        for local_batch, local_labels in validation_generator:\\n            # Transfer to GPU\\n            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\\n\\n            # Model computations\\n            '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True # ca marche pas chez moi,peut etre qu'il faut que j'installe un truc\n",
    "\n",
    "# Parameters, num_workers c'est les threads\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "\n",
    "max_epochs = 1 # j'ai mis a 1 pour le test pour aller vite\n",
    "\n",
    "# Datasets\n",
    "train, test = u.split(\"/home/celina/data/dsp/\",frames = True, percentage = 0.8, use= 0.001) # returns a split of the frames 80% train, 20% test, for 80% of the data\n",
    "\n",
    "# Generators\n",
    "directory = \"/home/celina/data/dsp/\"\n",
    "t = transforms.Compose([u.Reshape(), u.Rescale((224,224)), u.ToTensor()]) # composition of transformations\n",
    "training_set = u.Data(directory,train,transform = t) # open the dataset\n",
    "training_generator = DataLoader(training_set, **params)\n",
    "\n",
    "\n",
    "validation_set = u.Data(directory,test,transform = t) # open the dataset\n",
    "validation_generator = DataLoader(validation_set, **params)\n",
    "\n",
    "\"\"\"# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True # ca marche pas chez moi,peut etre qu'il faut que j'installe un truc\n",
    "\n",
    "# Parameters, num_workers c'est les threads\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "\n",
    "max_epochs = 1 # j'ai mis a 1 pour le test pour aller vite\n",
    "\n",
    "# Datasets\n",
    "train, test = u.split(\"/home/celina/data/dsp/\",frames = True, percentage = 0.8, use= 0.001) # returns a split of the frames 80% train, 20% test, for 80% of the data\n",
    "\n",
    "# Generators\n",
    "directory = \"/home/celina/data/dsp/\"\n",
    "t = transforms.Compose([u.Reshape(), u.Rescale((224,224)), u.ToTensor()]) # composition of transformations\n",
    "training_set = u.Data(directory,train,transform = t) # open the dataset\n",
    "training_generator = DataLoader(training_set, **params)\n",
    "\n",
    "\n",
    "validation_set = u.Data(directory,test,transform = t) # open the dataset\n",
    "validation_generator = DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,labels in training_generator:\n",
    "    sample_i = images[0]\n",
    "    sample_label = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9]], dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-38292fb52f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True # ca marche pas chez moi,peut etre qu'il faut que j'installe un truc\n",
    "\n",
    "# Parameters, num_workers c'est les threads\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "\n",
    "max_epochs = 1 # j'ai mis a 1 pour le test pour aller vite\n",
    "\n",
    "# Datasets\n",
    "train, test = u.split(\"/home/celina/data/dsp/\",frames = True, percentage = 0.8, use= 0.001) # returns a split of the frames 80% train, 20% test, for 80% of the data\n",
    "\n",
    "# Generators\n",
    "directory = \"/home/celina/data/dsp/\"\n",
    "t = transforms.Compose([u.Reshape(), u.Rescale((224,224)), u.ToTensor()]) # composition of transformations\n",
    "training_set = u.Data(directory,train,transform = t) # open the dataset\n",
    "training_generator = DataLoader(training_set, **params)\n",
    "\n",
    "\n",
    "validation_set = u.Data(directory,test,transform = t) # open the dataset\n",
    "validation_generator = DataLoader(validation_set, **params)\n",
    "\n",
    "model = cnn.Net().double()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Train the model\n",
    "total_step = len(training_generator)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(training_generator):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, np.argmax(labels,axis=1))\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == np.argmax(labels,axis=1)).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
